"""
RSS è·å–ä¸è§£ææ¨¡å—
è´Ÿè´£ä¸‹è½½ RSS XML å¹¶è§£æå‡ºç›®æ ‡æ—¥æœŸçš„å†…å®¹
æ”¯æŒå¤šæºå¹¶è¡ŒæŠ“å–å’Œå…³é”®è¯è¿‡æ»¤
"""
import feedparser
import requests
from datetime import datetime, timezone, timedelta
from typing import Optional, Dict, List, Any
from dateutil import parser as date_parser
import re
import concurrent.futures

from src.config import RSS_URL, RSS_SOURCES, RSS_TIMEOUT, KEYWORDS


class RSSFetcher:
    """RSS è·å–å™¨ - æ”¯æŒå¤šæºå¹¶è¡ŒæŠ“å–"""

    def __init__(self, rss_url: str = None, rss_sources: List[str] = None):
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å•ä¸ª URLï¼Œå¦åˆ™ä½¿ç”¨å¤šæºé…ç½®
        self.rss_url = rss_url or RSS_URL
        self.rss_sources = rss_sources or RSS_SOURCES
        self.timeout = RSS_TIMEOUT
        self._feed_data = None
        self._all_feeds = []  # å­˜å‚¨å¤šä¸ªæºçš„æ•°æ®

    def fetch(self) -> feedparser.FeedParserDict:
        """ä¸‹è½½å¹¶è§£æ RSS"""
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ RSS: {self.rss_url}")

        try:
            response = requests.get(
                self.rss_url,
                timeout=self.timeout,
                headers={
                    "User-Agent": "Mozilla/5.0 (compatible; AI-Daily/1.0)"
                }
            )
            response.raise_for_status()

            # ä½¿ç”¨ feedparser è§£æ
            feed = feedparser.parse(response.content)

            if feed.bozo:
                print(f"âš ï¸ RSS è§£æè­¦å‘Š: {feed.bozo_exception}")

            print(f"âœ… RSS ä¸‹è½½æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡èµ„è®¯")
            self._feed_data = feed
            return feed

        except requests.RequestException as e:
            raise Exception(f"RSS ä¸‹è½½å¤±è´¥: {e}")
        except Exception as e:
            raise Exception(f"RSS è§£æå¤±è´¥: {e}")

    def get_all_entries(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ¡ç›®"""
        if not self._feed_data:
            self.fetch()
        return self._feed_data.entries

    def get_content_by_date(self, target_date: str, feed: feedparser.FeedParserDict = None) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®æ—¥æœŸè·å–èµ„è®¯å†…å®¹

        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼: YYYY-MM-DD
            feed: RSS æ•°æ®ï¼Œå¦‚æœä¸ºç©ºåˆ™é‡æ–°è·å–

        Returns:
            åŒ¹é…çš„æ¡ç›®ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
        """
        if feed is None:
            feed = self.fetch()

        # è§£æç›®æ ‡æ—¥æœŸ
        try:
            target_dt = datetime.strptime(target_date, "%Y-%m-%d")
            target_dt = target_dt.replace(tzinfo=timezone.utc)
        except ValueError:
            raise ValueError(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {target_date}ï¼ŒæœŸæœ›æ ¼å¼: YYYY-MM-DD")

        print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾æ—¥æœŸ: {target_date}")

        # å°è¯•å¤šç§æ–¹å¼åŒ¹é…æ—¥æœŸ
        for entry in feed.entries:
            # æ–¹æ³•1: æ£€æŸ¥ pubDate
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                pub_dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                if self._is_same_day(pub_dt, target_dt):
                    return self._extract_entry_content(entry)

            # æ–¹æ³•2: ä» link ä¸­æå–æ—¥æœŸ (æ ¼å¼: .../issues/YY-MM-DD-slug/)
            if hasattr(entry, 'link'):
                date_from_link = self._extract_date_from_link(entry.link)
                if date_from_link and date_from_link == target_date:
                    return self._extract_entry_content(entry)

        print(f"âŒ æœªæ‰¾åˆ°æ—¥æœŸ {target_date} çš„èµ„è®¯")
        return None

    def _is_same_day(self, dt1: datetime, dt2: datetime) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªæ—¥æœŸæ˜¯å¦æ˜¯åŒä¸€å¤©"""
        return (dt1.year, dt1.month, dt1.day) == (dt2.year, dt2.month, dt2.day)

    def _extract_date_from_link(self, link: str) -> Optional[str]:
        """ä»é“¾æ¥ä¸­æå–æ—¥æœŸï¼Œæ ¼å¼: YY-MM-DD æˆ– YYYY-MM-DD"""
        # åŒ¹é… /issues/26-01-13- æˆ– /issues/2026-01-13- æ ¼å¼
        patterns = [
            r'/issues/(\d{2})-(\d{2})-(\d{2})-',  # YY-MM-DD
            r'/issues/(\d{4})-(\d{2})-(\d{2})-',  # YYYY-MM-DD
        ]

        for pattern in patterns:
            match = re.search(pattern, link)
            if match:
                year, month, day = match.groups()
                # å¦‚æœæ˜¯ä¸¤ä½å¹´ä»½ï¼Œè½¬æ¢ä¸ºå››ä½
                if len(year) == 2:
                    year = "20" + year
                return f"{year}-{month}-{day}"

        return None

    def _extract_entry_content(self, entry) -> Dict[str, Any]:
        """æå–æ¡ç›®å†…å®¹"""
        content = {
            "title": "",
            "link": "",
            "guid": "",
            "description": "",
            "content": "",
            "pubDate": ""
        }

        # æå–æ ‡é¢˜
        content["title"] = entry.get("title", "")

        # æå–é“¾æ¥
        content["link"] = entry.get("link", "")

        # æå– GUID
        content["guid"] = entry.get("id", entry.get("guid", content["link"]))

        # æå–æè¿°
        content["description"] = entry.get("description", "")

        # æå–å®Œæ•´å†…å®¹
        if hasattr(entry, 'content') and entry.content:
            content["content"] = entry.content[0].get('value', '')
        elif hasattr(entry, 'summary'):
            content["content"] = entry.summary
        else:
            content["content"] = content["description"]

        # æå–å‘å¸ƒæ—¥æœŸ
        if hasattr(entry, 'published'):
            content["pubDate"] = entry.published
        elif hasattr(entry, 'updated'):
            content["pubDate"] = entry.updated

        # æ¸…ç† HTML å®ä½“
        content["content"] = content["content"].replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')

        return content

    def get_latest_date(self, feed: feedparser.FeedParserDict = None) -> Optional[str]:
        """è·å–æœ€æ–°çš„èµ„è®¯æ—¥æœŸ"""
        if feed is None:
            feed = self.fetch()

        if not feed.entries:
            return None

        # è·å–ç¬¬ä¸€æ¡çš„æ—¥æœŸ
        entry = feed.entries[0]

        # å°è¯•ä» link ä¸­æå–
        if hasattr(entry, 'link'):
            date_from_link = self._extract_date_from_link(entry.link)
            if date_from_link:
                return date_from_link

        # å°è¯•ä» pubDate ä¸­æå–
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
            return dt.strftime("%Y-%m-%d")

        return None

    def get_date_range(self, feed: feedparser.FeedParserDict = None) -> tuple:
        """è·å– RSS ä¸­çš„æ—¥æœŸèŒƒå›´"""
        if feed is None:
            feed = self.fetch()

        if not feed.entries:
            return None, None

        dates = []
        for entry in feed.entries:
            if hasattr(entry, 'link'):
                date_from_link = self._extract_date_from_link(entry.link)
                if date_from_link:
                    dates.append(date_from_link)

        if not dates:
            return None, None

        return min(dates), max(dates)

    # ============================================================================
    # å¤šæºå¹¶è¡ŒæŠ“å–åŠŸèƒ½
    # ============================================================================

    def fetch_multiple(self) -> List[feedparser.FeedParserDict]:
        """
        å¹¶è¡ŒæŠ“å–å¤šä¸ª RSS æº

        Returns:
            æˆåŠŸæŠ“å–çš„ feed åˆ—è¡¨
        """
        # å¦‚æœè®¾ç½®äº†å•ä¸ª URLï¼Œä½¿ç”¨æ—§é€»è¾‘
        if self.rss_url:
            feed = self.fetch()
            return [feed] if feed else []

        print(f"ğŸ“¥ æ­£åœ¨å¹¶è¡ŒæŠ“å– {len(self.rss_sources)} ä¸ª RSS æº...")
        print(f"   æºåˆ—è¡¨: {self.rss_sources}")
        print()

        successful_feeds = []
        failed_sources = []

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡ŒæŠ“å–
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤æ‰€æœ‰æŠ“å–ä»»åŠ¡
            future_to_url = {
                executor.submit(self._fetch_single, url): url
                for url in self.rss_sources
            }

            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    feed = future.result()
                    if feed and feed.entries:
                        successful_feeds.append(feed)
                        print(f"   âœ… {url[:50]}... ({len(feed.entries)} æ¡)")
                    else:
                        failed_sources.append(url)
                        print(f"   âš ï¸ {url[:50]}... (æ— å†…å®¹)")
                except Exception as e:
                    failed_sources.append(url)
                    print(f"   âŒ {url[:50]}... ({str(e)[:30]})")

        print()
        print(f"âœ… æˆåŠŸæŠ“å– {len(successful_feeds)} ä¸ªæº")

        if failed_sources:
            print(f"âš ï¸ å¤±è´¥ {len(failed_sources)} ä¸ªæº")

        self._all_feeds = successful_feeds
        return successful_feeds

    def _fetch_single(self, url: str) -> Optional[feedparser.FeedParserDict]:
        """æŠ“å–å•ä¸ª RSS æº"""
        try:
            response = requests.get(
                url,
                timeout=self.timeout,
                headers={
                    "User-Agent": "Mozilla/5.0 (compatible; AI-Daily/1.0)"
                }
            )
            response.raise_for_status()
            return feedparser.parse(response.content)
        except Exception:
            return None

    def get_all_entries_from_sources(self) -> List[Dict[str, Any]]:
        """
        ä»æ‰€æœ‰æºè·å–æ‰€æœ‰æ¡ç›®ï¼Œå»é‡å¹¶æ’åº

        Returns:
            åˆå¹¶åçš„æ‰€æœ‰æ¡ç›®åˆ—è¡¨
        """
        if not self._all_feeds:
            self.fetch_multiple()

        # ä½¿ç”¨ URL å»é‡
        seen_urls = set()
        all_entries = []

        for feed in self._all_feeds:
            for entry in feed.entries:
                url = entry.get('link', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    # æ·»åŠ æ¥æºä¿¡æ¯
                    entry._source = getattr(feed, 'feed', {}).get('title', url)
                    all_entries.append(entry)

        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        all_entries.sort(
            key=lambda e: e.get('published_parsed', (0, 0, 0, 0, 0, 0, 0, 0, 0)),
            reverse=True
        )

        print(f"ğŸ“Š åˆå¹¶åå…± {len(all_entries)} æ¡ä¸é‡å¤èµ„è®¯")
        return all_entries

    def filter_by_keywords(self, entries: List[Dict[str, Any]], keywords: List[str] = None) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å…³é”®è¯è¿‡æ»¤æ¡ç›®

        Args:
            entries: æ¡ç›®åˆ—è¡¨
            keywords: å…³é”®è¯åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ KEYWORDS

        Returns:
            è¿‡æ»¤åçš„æ¡ç›®åˆ—è¡¨
        """
        if keywords is None:
            keywords = KEYWORDS

        # å¦‚æœå…³é”®è¯åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›æ‰€æœ‰æ¡ç›®
        if not keywords:
            return entries

        filtered = []
        for entry in entries:
            if self._matches_keywords(entry, keywords):
                filtered.append(entry)

        if filtered:
            print(f"ğŸ” å…³é”®è¯è¿‡æ»¤: {len(filtered)}/{len(entries)} æ¡åŒ¹é…")

        return filtered

    def _matches_keywords(self, entry: Dict[str, Any], keywords: List[str]) -> bool:
        """æ£€æŸ¥æ¡ç›®æ˜¯å¦åŒ¹é…ä»»ä½•å…³é”®è¯"""
        # è·å–å¯æœç´¢çš„æ–‡æœ¬
        title = entry.get('title', '').lower()
        summary = entry.get('summary', entry.get('description', '')).lower()
        tags = ' '.join(
            tag.get('term', '') for tag in entry.get('tags', [])
        ).lower()

        combined_text = f"{title} {summary} {tags}"

        # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        for keyword in keywords:
            if keyword.lower() in combined_text:
                return True

        return False

    def get_todays_entries(self, days_back: int = 1) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘ N å¤©çš„æ¡ç›®ï¼ˆæ¥è‡ªæ‰€æœ‰æºï¼Œå¸¦å…³é”®è¯è¿‡æ»¤ï¼‰

        Args:
            days_back: å›æº¯å¤©æ•°

        Returns:
            åŒ¹é…çš„æ¡ç›®åˆ—è¡¨
        """
        all_entries = self.get_all_entries_from_sources()

        # åº”ç”¨å…³é”®è¯è¿‡æ»¤
        filtered_entries = self.filter_by_keywords(all_entries)

        # æŒ‰æ—¥æœŸè¿‡æ»¤
        target_date = datetime.now(timezone.utc) - timedelta(days=days_back)
        cutoff_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0)

        recent_entries = []
        for entry in filtered_entries:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                entry_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                if entry_date >= cutoff_date:
                    recent_entries.append(entry)

        print(f"ğŸ“… æœ€è¿‘ {days_back} å¤©å†…æœ‰ {len(recent_entries)} æ¡ç›¸å…³èµ„è®¯")
        return recent_entries

    def get_content_by_date_from_sources(self, target_date: str) -> List[Dict[str, Any]]:
        """
        ä»æ‰€æœ‰æºè·å–æŒ‡å®šæ—¥æœŸçš„å†…å®¹

        Args:
            target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)

        Returns:
            åŒ¹é…çš„æ‰€æœ‰æ¡ç›®åˆ—è¡¨
        """
        all_entries = self.get_all_entries_from_sources()

        # åº”ç”¨å…³é”®è¯è¿‡æ»¤
        filtered_entries = self.filter_by_keywords(all_entries)

        # è§£æç›®æ ‡æ—¥æœŸ
        try:
            target_dt = datetime.strptime(target_date, "%Y-%m-%d")
            target_dt = target_dt.replace(tzinfo=timezone.utc)
        except ValueError:
            raise ValueError(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {target_date}")

        # æŸ¥æ‰¾åŒ¹é…æ—¥æœŸçš„æ¡ç›®
        matched_entries = []
        for entry in filtered_entries:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                pub_dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                if self._is_same_day(pub_dt, target_dt):
                    matched_entries.append(entry)

        print(f"ğŸ“… {target_date} æ‰¾åˆ° {len(matched_entries)} æ¡ç›¸å…³èµ„è®¯")
        return matched_entries


def fetch_rss_content(target_date: str) -> Optional[Dict[str, Any]]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–æŒ‡å®šæ—¥æœŸçš„ RSS å†…å®¹"""
    fetcher = RSSFetcher()
    feed = fetcher.fetch()
    return fetcher.get_content_by_date(target_date, feed)
